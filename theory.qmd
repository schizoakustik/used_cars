
```{r}
#| label: theory-setup
#| include: false

library(tidyverse)
library(plotly)
library(pracma)
```

# Theory

## Statistical learning

In short, statistical learning is the process of using statistical methods and mathematical models to understand data [@islr, p. 1]. This process can be used to predict values given a set of variables, or to gain a better understanding of how those variables relate to the wanted value, also called *inference*. Often, the goal might be both prediction and inference.

One of the simplest, yet most powerful, methods of statistical learning is *linear regression*.

## Linear regression

### Simple linear regression

At the heart of linear regression lies the equation of the straight line (@eq-line)

$$ y = b + mx$$ {#eq-line}

where $b$ denotes the *intercept* where the line crosses the y-axis, and $m$ denotes the *slope* of the line.

In simple linear regression we assume that the relationship between the response variable $Y$ and the independent variable $X$ is approximately linear, which can be expressed as (@eq-linreg-1). [@islr, p. 61]

$$ Y \approx \beta_0 + \beta_1 X$$ {#eq-linreg-1}

Here, $\beta_0$ denotes the intercept and $\beta_1$ denotes the slope of the line. Using training data, we can estimate the *coefficients* $\hat{\beta_0}$ and $\hat{\beta_1}$ and predict a certain value of $Y$ by computing 

$$ \hat{y} = \hat{\beta_0} + \hat{\beta_1} x. $$ {#eq-linreg-2}

#### Estimating coefficients

The coefficients can be estimated using a number of approaches, but the most common method is the *least squares* criterion.

The goal is to find values for $\beta_0$ and $\beta_1$ such that the difference between the observed response value and our predicted response value, the *residual*, is as small as possible. The *i*th residual $e_i$ can be expressed as $e_i = y_i - \hat{y}_i$. 

#### Dummy variables
The linear regression model works with numbers, but the data collected has some columns that contain categorical data, such as `transmission`, `type`, and the `make` column. To use these in a linear regression context, they must be transformed into numbers. The most common way is to use the concept *dummy variables*, where each category in the column becomes a column of its own, and each observation in this category gets a 1 in that column, and a 0 in the others.

In a data set with many different categories, this also introduces many predictors.

#### Evaluating model performance

To assess how well a model fits the data, two measures are commonly used: the *residual standard error* ($RSE$), and the $R^2$ statistic.

##### The residual standard error

The $RSE$ is an estimate of the standard deviation of $\epsilon$ [@islr, p. 69] and is computed by

$$ RSE = \sqrt{\dfrac{1}{n - 2}RSS}. $$ {#eq-rse}

If the predicted values of the model are close to the observed values, the $RSE$ will be small and the model fits the data well.

##### The $R^2$ statistic

The $RSE$ is measured in the units of $Y$. This makes it hard to interpret what a good value of the $RSE$ would be. [@islr, p. 70]. The $R^2$ statistic, on the other hand, is a *proportion*, and as such always takes on a value between 0 and 1.

The $R^2$ statistic describes how much of the total variance in the response $Y$ that can be explained by using $X$ as a predictor. The formula used is seen in @eq-r2.

$$ R^2 = \dfrac{TSS-RSS}{TSS} = 1 - \dfrac{RSS}{TSS} $$ {#eq-r2}

### Multiple linear regression
So far we have been using a single predictor $X_1$ to predict the response $Y$. In *multiple linear regression*, additional predictors are added to form the model

$$ Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_p X_p + \epsilon $$ {#eq-multi-linreg-1}

where $X_j$ represents the *j*th predictor, and $\beta_j$ represents the effect of $X_j$ on $Y$, or, in other words, the *average* effect on $Y$ on a one unit increase in $X_j$, given that *all other predictors remain fixed*.

The coefficients $\beta_0, \beta_1, \ldots, \beta_p$ are estimated in the same way as in simple linear regression, using the least squares method.

#### The $F$-statistic

To find out if any of our predictors actually affect the response variable, we can perform a *hypothesis test*. Our null hypothesis $H_0$ is that none of the predictors are related to the outcome:

$$ H_0: \beta_1 = \beta_2 = \cdots = \beta_p = 0 $$

which we test against the alternative hypothesis $H_a$

$$ H_a: \text{at least one } \beta_j \text{ is non-zero.} $$

To perform the test, we compute the *F-statistic*,

$$ F = \dfrac{(TSS-RSS)/p}{RSS/(n-p-1).} $$ {#eq-f-stat}

If $H_0$ is true, the expected value of the $F$-statistic is close to $1$. If the alternative hypothesis is true, we expect $F$ to be greater than $1$.

#### Adjusted $R^2$

Adding variables to a model leads to an increase in the $R^2$ statistic. [@islr, p. 235] This might be a problem, since we can't tell from looking at $R^2$ if the predictors added actually makes the model better. One approach is to add a penalty to @eq-r2 so that only predictors that lower the RSS also increase $R^2$. This is called the *adjusted $R^2$* and the formula is

$$ \text{Adjusted } R^2 = 1 - \dfrac{RSS/(n-d-1)}{TSS/(n-1)}. $$ {#eq-adj-r2}

#### Variable selection
The use of statistical methods for variable selection is not uncontroversial. Rob J Hyndman writes on his blog that

> [s]tatistical significance is not usually a good basis for determining whether a variable should be included in a model, despite the fact that many people who should know better use them for exactly this purpose. [@hyndman]

Domain knowledge should always be an important part of variable selection.

#### Potential problems

In *Introduction to Statistical Learning*, James and co-authors list six of the most common potential problems when fitting linear regression models [@islr, p. 93]. These are:

1. Non-linearity of the response-predictor relationships
2. Correlation of error terms
3. Non-constant variance of error terms
4. Outliers
5. High-leverage points
6. Collinearity

##### Non-linearity of the response-predictor relationships
The linear regression method assumes a linear relationship between predictors and response. If this is not the case, the predictions and conclusions drawn from the data are suspect. Non-linear transformations of the predictors such as squaring and taking logarithms might be a solution.

##### Correlation of error terms
When observations in the data share features that are not among the predictors, the error terms might be correlated, which leads to confidence intervals that are too narrow. Good experimental design is key.

##### Non-constant variance of error terms
If the variance of the error terms are related to the value of the response, we are dealing with *heteroscedasticity*. One solution is to transform the response variable with a concave function to regulate larger response values harder.

##### Outliers
Outliers are values in the training data that differ a lot from the predicted values. An outlier might be a result of faulty data, but can also be a sign of missing predictors.

##### High-leverage points
If an outlier is defined by having an unusual $y_i$-value, a high-leverage point is defined by having an unusual $x_i$-value. They can be a bit tricky to identify i multiple linear models.

##### Collinearity
Having two, or more, predictors that are correlated results in collinearity or multicollinearity, respectively. Collinearity makes it harder to determine the affect of the individual predictors, and might lead to reduced power of the hypothesis test that $\beta_j = 0$. The *variance inflation factor*, VIF, is the ratio of the variance of a predictor when fitted on the full model, divided by the variance when fitted on its own. Dropping correlated columns might lead to less collinearity.

## Model evaulation

### Train/test set splitting
When training machine learning models, one problem is that the model can get to aquainted with the data it's training on. This is called *overfitting* and can lead to a model that looks like it is performing well, but then predicts poorly when introduced to data it has not yet seen.

For this purpose, it is common to split the data into two sets, one training set, and one test set that is put aside and not used until the very end of the process.

Since the test set is not to be used until the end, sometimes the training data is split once more into a training and a validation set to use when comparing different models.

### K-fold cross validation
Instead of a validation set, one can use an approach called *k-fold cross validation*. The training data is split into $k$ number of folds. Then, one fold is held out to use for validation, and the model is trained on the remaining folds. This process is repeated until each fold has been used as a validation set, and the results are averaged to get the final metrics.