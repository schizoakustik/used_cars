```{r}
#| label: setup
#| include: false

library(tidyverse)
library(tidymodels)
library(skimr)
library(patchwork)
library(ggcorrplot)
library(GGally)
library(ggthemes)
library(viridis)
library(car)

options(scipen = 12, digits = 5)
theme_set(theme_bw())
set.seed(42)

df_prep <- readRDS(file = "data/df_prep.Rdata")

data_split <- initial_split(df_prep, prop = .8)

train_data <- training(data_split)
test_data <- testing(data_split)
```

# Results and discussion
## EDA {#sec-eda}
The raw data was preprocessed as described in the Data Preprocessing section of the Theory chapter (-@sec-preprocess). Here we take a closer look at the data.

### Overview
```{r}
#| label: tbl-data-overview
#| tbl-cap: "Overview of training data"
#| echo: false

train_data
```
The entire data set consists of 7 661 observations with 18 features each. Each observation is an ad for a used car.

### Missing data
Several of the observations in the raw data had missing values in the `price` column. Those where filtered out at the very start as `price` is our target value.

There are also some 700 observations that are have missing values in the `motorsize` column. Most of these are electric cars (see @fig-missing-motorsize). I want to keep the electric cars in the data set so I don't want to remove those observations all together.

There does seem to be some sort of relationship between motor size and price. (See @fig-motorsize-vs-price). Removing the column might have a negative impact on the model's ability to predict prices, but keeping it leaves me with a whole lot of missing values, and issues with electric cars. One way might be to impute the column with the mean or median of the motor sizes, but since electric cars actually don't have a motor size, that seems wrong.

My final decision was to remove the `motorsize` column.

```{r}
#| label: fig-missing-motorsize
#| fig-cap: "Proportion of missing motor size values by fuel type"
#| echo: false

subs <- read.csv("data/car_ads_data_02.csv") |>
  dplyr::filter(!is.na(price)) |>
  dplyr::filter(price >= 20000 & price <= 500000) |>
  mutate(across(everything(), str_trim)) |>
  mutate(across(everything(), ~ na_if(.x, "NA"))) |>
  mutate(across(c(motorsize, price), as.numeric))

bind_cols(
  subs |> 
    group_by(fuel) |>
    summarize(count = n()) |>
    ungroup(),
  subs |>
    group_by(fuel) |>
    select(fuel, motorsize) |>
    summarize(na = sum(is.na(motorsize))) |>
    ungroup() |>
    select(-fuel)
) |> mutate(prop = na/count) |>
  select(fuel, prop) |>
  ggplot(aes(x = fuel, y = prop, fill = fuel)) +
  geom_col() + 
  theme(legend.position = "none") +
  labs(title = "Proportion of missing motor size values by fuel type", x = NULL, y = "Proportion")

```

```{r}
#| label: fig-motorsize-vs-price
#| fig-cap: "Motor size vs. price with an added regression line."
#| echo: false

subs_no_na <- subs |> filter(!is.na(motorsize)) 

subs_no_na |>
  ggplot(aes(x = motorsize, y = price)) + 
  geom_point() + 
  geom_smooth() +
  theme(axis.text.x = element_blank(), axis.text.y = element_blank()) + 
  labs(title = "Motorsize vs. price")
```

### Other columns with missing data 

Three remaining columns had missing values. They were all categorical values and it didn't make sense to impute them with the most common values. Since there would be a maximum loss of 123 observations, the rows with the missing values were removed.

### Handling uncommon values

Some values in the `make` category were only present in a few observations. This increases the risk of having values that are only present in the training or test data sets. At least one make was present only in the single observation. Even when using a stratified split, that observation would end up in either the training or the test set.

I decided to keep them in the data, but group them together under the "Other" name. The cutoff is set to be less than the number of columns in the data set, that is, 18.

## Results

With the data prepared and split, it's time to take a look at it. 

### Numerical variables
Figure @fig-scatter-matrix-num shows the numerical columns plotted against each other. `year` has been replaced by `age`. `days_in_traffic` shows a strong correlation with `age`, which is no surprise. This might result in collinearity when training the model.

```{r}
#| label: fig-scatter-matrix-num
#| fig-cap: "Scatter matrix plot of numerical columns"
#| echo: false
#| 
train_data |> select(where(is.numeric), - year) |> 
  ggpairs() + 
  labs(title = "Scatter matrix plot over numerical columns")
```

The predictors that seem to have the strongest correlations to our `price` response variable are `age` at -0.701, `days_in_traffic` at -0.711, and `hp` at 0.59.

Interestingly, the `miles_per_day` column, which is a measure of both how old the car is, and how much use it has seen, does not seem to have much impact on the price.

@tbl-vif-1 shows the VIF scores of the columns. As expected, `days_in_traffic` and `age` both have a score higher than 5.

```{r}
#| label: tbl-vif-1
#| tbl-cap: "VIF scores of the `age`, `days_in_traffic` and `hp` columns."
#| echo: false


lm(price ~ days_in_traffic + age + hp, data = train_data) |> 
vif() |> 
as.data.frame() |> 
knitr::kable(col.names = c("Column", "VIF"))

```
I needed to choose between `days_in_traffic` and `age` since those columns where strongly correlated. Fitting two models with the two variables as the only predictor gave the results in @tbl-days-age.

```{r}
#| label: tbl-days-age
#| tbl-cap: "The $R^2$ of two linear models, fitted with the `age` and `days_in_traffic` variables."
#| echo: false

age_model <- lm(price ~ age, data = train_data) |> summary()
days_model <- lm(price ~ days_in_traffic, data = train_data) |> summary()

bind_rows(
  tibble(Age = age_model$r.squared, `Days in traffic` = days_model$r.squared)
) |> t() |>
knitr::kable(col.names = c("Column", "R squared"))
```
`days_in_traffic` results in a slightly higher $R^2$. and was thus chosen for use in the model.

### Qualitative variables

The data set has qualitative variables that range from two unique categories (`transmission`), to 565 (`make_model`). @tbl-cat-levels shows the number of categories in the qualitative columns.

```{r}
#| label: tbl-cat-levels
#| tbl-cap: "Number of categories in qualitative columns"
#| echo: false

train_data |> 
  summarise(across(where(is.factor), nlevels)) |>
  knitr::kable()
```
Including the `make_model` column would introduce 564 new predictors to the model. Fitting a model using just that column, 65 have a p-value below 0.05. I could just keep those, but the model becomes harder to interpret. `make_model` will not be part of the final model.

```{r}
#| label: sig-make-model
#| echo: false
sig_make_model_count <- train_data |>
  nest() |>
  mutate(
    fit = map(data, ~ lm(price ~ make_model, data = .x)),
    tidied = map(fit, tidy),
    glanced = map(fit, glance),
    augmented = map(fit, augment)
  ) |> 
  select(tidied) |> 
  unnest(tidied) |>
  filter(p.value <= 0.05) |>
  count()

print(paste("Number of make_model predictors with a p-value < 0.05: ", sig_make_model_count))
```

#### Which car makes have a significant impact on the price?

Of the 63 unique car makes in the original data set, 28 had 18 observations or fewer. As mentioned, these are grouped together under the "Other" name, which gives us 36 unique values for the `make` category. One question I wanted to ask was, which makes (if any) seem to have an impact on the asking price for the car? [Figure @fig-significant-makes] shows the estimated coefficients of the 16 car makes that have a statistically significant effect on the price.


```{r}
#| label: fig-significant-makes
#| fig-cap: "Estimated coefficients of car makes with a p-value < 0.05"
#| echo: false

sig_makes <-
  train_data |>
  nest() |>
  mutate(
    fit = map(data, ~ lm(price ~ make, data = .x)),
    tidied = map(fit, tidy),
    glanced = map(fit, glance),
    augmented = map(fit, augment)
)

sig_makes |> 
  select(tidied) |> 
  unnest(tidied) |>
  filter(p.value <= 0.05) |>
  arrange(estimate) |>
  mutate(term = factor(term, levels = term)) |>
  ggplot(aes(x = estimate, y = term, fill = term)) +
  geom_col() +
  scale_y_discrete(label = ~ str_replace_all(.x, c("make" = "", "\\(Intercept\\)" = "Aston Martin"))) +
  theme(legend.position = "none") +
  labs(title = "Estimated coefficients by car make", x = "Estimated coefficients", y = NULL)

```
Any other car make has an average price close enough to the mean price that it can't be said that the make alone has any effect on the price.

This brings the question, should the model contain the make predictor or not? Since the goal is not to make the best prediction model but rather to make inference, including the make column has already told us that some make do affect the price of the car, while others do not.

[@tbl-sig-makes] shows a quick look at what happens with the adjusted $R^2$ value when all non-significant makes are grouped into the `Other` category.

```{r}
#| label: tbl-sig-makes
#| tbl-cap: "Adjusted $R^2$ for models with all makes and only significant makes"
#| echo: false

sig_make_names <- sig_makes |> 
  select(tidied) |> 
  unnest(tidied) |>
  filter(p.value <= 0.05) |>
  mutate(term = str_replace_all(term, c("make" = "", "\\(Intercept\\)" = "Aston Martin"))) |>
  select(term)

train_data_only_sig_makes <- 
  rows_update(
    train_data, 
    train_data |> 
      group_by(make) |>
      filter(!(make %in% sig_make_names$term)) |>
      mutate(make = factor("Other")),
    by = "id"
  ) |>
  mutate(make = as.factor(make))

all_make_model <- lm(price ~ make, data = train_data) |> summary()
sig_make_model <- lm(price ~ make, data = train_data_only_sig_makes) |> summary()

bind_cols(
  tibble("All makes" = all_make_model$adj.r.squared, "Only significant makes" = sig_make_model$adj.r.squared)
) |> t() |>
knitr::kable(col.names = c("Model", "R squared"))

```

The adjusted $R^2$ actually goes down. The `make` column is left as it is.

#### Tesla is driving up the prices of electric cars
From @fig-significant-makes we see that the car make that has the highest effect on price is Tesla. 

Since Tesla exclusively makes electric cars, the question is how this might affect a model's ability to predict the prices of that kind of vehicle.

A vast majority, almost 40%, of the electric cars in the data set are Teslas. They might not be the most expensive (see @fig-electric-cars-2), but the electric car makes with higher average prices are not nearly as well represented in the data.

```{r}
#| label: fig-electric-cars
#| fig-cap: "Proportions and average prices of electric cars by make"
#| fig-subcap: 
#|    - "Proportion of electric cars by make"
#|    - "Average prices of electric cars by make"
#| layout-ncol: 2
#| column: page
#| echo: false

train_data_fuel <- train_data |> 
  filter(fuel == "El")

train_data_fuel |>
  group_by(make) |>
  summarize(n = n()) |>
  mutate(freq = n / sum(n)) |>
  arrange(freq) |>
  mutate(make = factor(make, levels = make)) |>
  ggplot(aes(x = freq, y = make, fill = make)) +
  geom_col() + 
  scale_x_continuous(labels = scales::percent) + 
  labs(title = "Proportion of electric cars by make", x = "Proportion", y = NULL)

train_data_fuel |>
  group_by(make) |>
  summarize(avg_price = mean(price)) |>
  arrange(avg_price) |>
  mutate(make = factor(make, levels = make)) |>
  ggplot(aes(x = avg_price, y = make, fill = make)) +
  geom_col() +
  labs(title = "Average price of electric cars by make", x = "Average price", y = NULL)
```

[@fig-fuel-and-make] shows the coefficients for the `fuel` predictor, with confidence intervals.
The blue figures shows the coefficients in a model fitted without the `make` predictor, while the red figures shows them in a model where the `make` column was present. For `Hybrid`, `Diesel` and `Bensin` (gasoline), the figures overlap, and we can't say that including the `make` predictor has any significant effect on the `fuel` ceofficients. The `El` category, which is electric cars, tells another story. Here, the model without the `make` predictor has ha significantly higher coefficient than the other model. When `make` is accounted for, the `El` coefficient it not enough to drive up the prices of electric cars. This suggests that the `make` predictor should be left in the model.

```{r}
#| label: fig-fuel-and-make
#| fig-cap: "Confidence levels of coefficients for the fuel category, with and without taking the make category into account."
#| echo: false

ignore_make <- 
  lm(price ~ fuel, data = train_data) |> tidy()

account_for_make <- 
  lm(price ~ fuel + make, data = train_data) |> tidy()

both_models <- bind_rows(
  ignore_make |> mutate(make = "ignore"),
  account_for_make |> mutate(make = "account for make")
) 

both_models |>
  filter(!str_detect(term, "make")) |>
  ggplot(aes(estimate, term, color = make)) +
  geom_errorbar(aes(
    xmin = estimate - 1.96 * std.error, 
    xmax = estimate + 1.96 * std.error),
    alpha = .7) +
  geom_point(size = 3) +
  geom_vline(xintercept = 0, lty = 2, color = "gray50") +
  scale_y_discrete(label = ~ str_replace_all(.x, c("fuel" = "", "\\(Intercept\\)" = "Bensin"))) +
  labs(title = "Influence of make predictor on price estimates on fuel category", y = NULL, x = "Estimated prices")
```
#### Region
The data from SCB was joined to the ads data on an early stage, but neither the total amount of cars in the region, or the number of cars per 1000 residents had any impact on the price. In fact, the whole `region` column was left out.

### The final model
In the end, the predictors that made it into the final model were `days_in_traffic`, `hp`, `make`, `fuel`, `type`, `drive`, and `transmission`.

Some transformations were done. All numerical predictors were normalized, and the response value waslog transformed. [@fig-final-model] shows some evaluation plots, and [@tbl-final-r2] shows the $R^2$ for the final model.

```{r}
#| label: fig-final-model
#| fig-cap: "Residual plots of the final model"
#| fig-subcap:
#|  - "Histogram of residuals"
#|  - "Fitted values vs. standardized residuals"
#|  - "QQ Plot"
#|  - "Leverage vs. standardized residuals"
#| layout-ncol: 2
#| layout-nrow: 2
#| column: page
#| echo: false

# Create CV folds
cv_folds <- vfold_cv(train_data, v = 5)

# Create recipes
car_ads_rec <-
  recipe(price ~ ., data = train_data) |>
  update_role(id, new_role = "ID") |>
  update_role_requirements("ID", bake = FALSE) |>
  remove_role(all_of(
    c(
      "age",
      "year",
      "mileage",
      "miles_per_day", 
      "model",
      "make_model",
      "region",
      "color",
      "first_in_traffic"
      )), 
      old_role = "predictor") |>
  update_role_requirements("NA", bake = FALSE) |>
  step_normalize(all_numeric_predictors()) |>
  step_dummy(all_nominal_predictors()) |>
  step_log(price, skip = TRUE)

# car_ads_rec |>
#   prep() |>
#   bake(new_data = NULL)

# Build model
linreg_model <- 
  linear_reg()

# Create workflow
car_ads_wf <-
  workflow() |> 
    add_recipe(car_ads_rec) |>
    add_model(linreg_model)

# Fit model on folds
car_ads_fit <-
  car_ads_wf |>
  fit_resamples(
    resamples = cv_folds,
    metrics = metric_set(rmse, rsq, mae))

# Collect metrics
car_ads_metrics <- car_ads_fit |>
  collect_metrics() |>
  arrange(.metric)

# Fit on all data and extract parsnip object
car_ads_final_fit <- car_ads_wf |> 
  last_fit(split = data_split) |> 
  extract_fit_parsnip()

# Add predictions
car_ads_aug <- 
  augment(car_ads_final_fit$fit) |>
  mutate(price = ..y) |>
  select(price, .fitted:.std.resid)

# Plot results
ggplot(car_ads_aug, aes(.std.resid)) +
  geom_histogram(bins = 50) +
  theme_bw() +
  labs(subtitle = "Distribution of residuals", x = "Standardized residuals", y = NULL)

ggplot(car_ads_aug, aes(.fitted, .std.resid)) + 
  geom_point(alpha = .5) +
  geom_hline(yintercept = 0) +
  geom_smooth() + 
  theme_bw() +
  labs(subtitle = "Fitted values vs. standardized residuals", x = "Fitted values", y = "Standardized residuals")

ggplot(car_ads_aug, aes(sample = .std.resid)) + 
  geom_qq(alpha = .5) + 
  geom_qq_line() +
  theme_bw() +
  labs(subtitle = "QQ Plot", x = "Theoretical quantiles", y = "Standardized residuals")

ggplot(car_ads_aug, aes(.hat, .std.resid, size = .cooksd)) + 
  geom_point(alpha = .5) +
  geom_hline(yintercept = 0, lty = 2, col = "grey50") +
  # geom_smooth() + 
  theme_bw() +
  labs(subtitle = "Leverage vs. standardized residuals", x = "Leverage", y = "Standardized residuals")
```

```{r}
#| label: tbl-final-r2
#| tbl-cap: "$R^2$ of the final model"
#| echo: false

yardstick::rsq(car_ads_aug, truth = price, estimate = .fitted) |> 
select(.metric, .estimate) |>
knitr::kable(col.names = c("Metric", "Estimate"))

```

## Discussion
### Possible missing variables

One potentially important variable that is missing from the data is the price of the car when bought as new. Gathering that data given the size of the data set did not seem feasible.

It would also be interesting to add the free text descriptions to the data and perform textual analysis. I suspect that there are hidden factors in the descriptions, such as information about whether the car is registered for use, if it has had it's yearly inspection, and so on.

### Going further

While is is interesting to look at what affects the price of used cars using a snapshot like this, it would be even more interesting to collect the same data periodically and analyse it over time, to look for trends and compare it to other data, such as fuel prices.